{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, DenseVector([0.5, 10.0])),\n",
       " (0.0, DenseVector([1.5, 20.0])),\n",
       " (1.0, DenseVector([1.5, 30.0])),\n",
       " (0.0, DenseVector([3.5, 30.0])),\n",
       " (0.0, DenseVector([3.5, 40.0])),\n",
       " (1.0, DenseVector([3.5, 40.0]))]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
    "        (0.0, Vectors.dense(1.5, 20.0)),\n",
    "        (1.0, Vectors.dense(1.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 40.0)),\n",
    "        (1.0, Vectors.dense(3.5, 40.0))]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5,10.0]\n"
     ]
    }
   ],
   "source": [
    "print(Vectors.dense(0.5, 10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/20 11:55:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/20 11:55:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('app').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.5, 10.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>[1.5, 20.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[1.5, 30.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>[3.5, 30.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>[3.5, 40.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label     features\n",
       "0    0.0  [0.5, 10.0]\n",
       "1    0.0  [1.5, 20.0]\n",
       "2    1.0  [1.5, 30.0]\n",
       "3    0.0  [3.5, 30.0]\n",
       "4    0.0  [3.5, 40.0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pandas_df = pd.DataFrame(data=data, columns=['label', 'features'])\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, ['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/20 12:01:57 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)/ 1]\n",
      "org.apache.spark.SparkException: \n",
      "Error from python worker:\n",
      "  Error: pyspark does not support any application options.\n",
      "  \n",
      "  Usage: ./bin/pyspark [options]\n",
      "  \n",
      "  Options:\n",
      "    --master MASTER_URL         spark://host:port, mesos://host:port, yarn,\n",
      "                                k8s://https://host:port, or local (Default: local[*]).\n",
      "    --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\n",
      "                                on one of the worker machines inside the cluster (\"cluster\")\n",
      "                                (Default: client).\n",
      "    --class CLASS_NAME          Your application's main class (for Java / Scala apps).\n",
      "    --name NAME                 A name of your application.\n",
      "    --jars JARS                 Comma-separated list of jars to include on the driver\n",
      "                                and executor classpaths.\n",
      "    --packages                  Comma-separated list of maven coordinates of jars to include\n",
      "                                on the driver and executor classpaths. Will search the local\n",
      "                                maven repo, then maven central and any additional remote\n",
      "                                repositories given by --repositories. The format for the\n",
      "                                coordinates should be groupId:artifactId:version.\n",
      "    --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while\n",
      "                                resolving the dependencies provided in --packages to avoid\n",
      "                                dependency conflicts.\n",
      "    --repositories              Comma-separated list of additional remote repositories to\n",
      "                                search for the maven coordinates given with --packages.\n",
      "    --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place\n",
      "                                on the PYTHONPATH for Python apps.\n",
      "    --files FILES               Comma-separated list of files to be placed in the working\n",
      "                                directory of each executor. File paths of these files\n",
      "                                in executors can be accessed via SparkFiles.get(fileName).\n",
      "    --archives ARCHIVES         Comma-separated list of archives to be extracted into the\n",
      "                                working directory of each executor.\n",
      "  \n",
      "    --conf, -c PROP=VALUE       Arbitrary Spark configuration property.\n",
      "    --properties-file FILE      Path to a file from which to load extra properties. If not\n",
      "                                specified, this will look for conf/spark-defaults.conf.\n",
      "  \n",
      "    --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n",
      "    --driver-java-options       Extra Java options to pass to the driver.\n",
      "    --driver-library-path       Extra library path entries to pass to the driver.\n",
      "    --driver-class-path         Extra class path entries to pass to the driver. Note that\n",
      "                                jars added with --jars are automatically included in the\n",
      "                                classpath.\n",
      "  \n",
      "    --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n",
      "  \n",
      "    --proxy-user NAME           User to impersonate when submitting the application.\n",
      "                                This argument does not work with --principal / --keytab.\n",
      "  \n",
      "    --help, -h                  Show this help message and exit.\n",
      "    --verbose, -v               Print additional debug output.\n",
      "    --version,                  Print the version of current Spark.\n",
      "  \n",
      "   Spark Connect only:\n",
      "     --remote CONNECT_URL       URL to connect to the server for Spark Connect, e.g.,\n",
      "                                sc://host:port. --master and --deploy-mode cannot be set\n",
      "                                together with this option. This option is experimental, and\n",
      "                                might change between minor releases.\n",
      "  \n",
      "   Cluster deploy mode only:\n",
      "    --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n",
      "                                (Default: 1).\n",
      "  \n",
      "   Spark standalone or Mesos with cluster deploy mode only:\n",
      "    --supervise                 If given, restarts the driver on failure.\n",
      "  \n",
      "   Spark standalone, Mesos or K8s with cluster deploy mode only:\n",
      "    --kill SUBMISSION_ID        If given, kills the driver specified.\n",
      "    --status SUBMISSION_ID      If given, requests the status of the driver specified.\n",
      "  \n",
      "   Spark standalone, Mesos and Kubernetes only:\n",
      "    --total-executor-cores NUM  Total cores for all executors.\n",
      "  \n",
      "   Spark standalone, YARN and Kubernetes only:\n",
      "    --executor-cores NUM        Number of cores used by each executor. (Default: 1 in\n",
      "                                YARN and K8S modes, or all available cores on the worker\n",
      "                                in standalone mode).\n",
      "  \n",
      "   Spark on YARN and Kubernetes only:\n",
      "    --num-executors NUM         Number of executors to launch (Default: 2).\n",
      "                                If dynamic allocation is enabled, the initial number of\n",
      "                                executors will be at least NUM.\n",
      "    --principal PRINCIPAL       Principal to be used to login to KDC.\n",
      "    --keytab KEYTAB             The full path to the file that contains the keytab for the\n",
      "                                principal specified above.\n",
      "  \n",
      "   Spark on YARN only:\n",
      "    --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n",
      "        \n",
      "PYTHONPATH was:\n",
      "  /Users/asaucedov/Downloads/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip:/Users/asaucedov/Downloads/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip:/Users/asaucedov/Downloads/spark-3.5.0-bin-hadoop3/jars/spark-core_2.12-3.5.0.jar\n",
      "org.apache.spark.SparkException: EOFException occurred while reading the port number from pyspark.daemon's stdout and terminated with code: 1..\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.eofExceptionWhileReadPortNumberError(SparkCoreErrors.scala:53)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:250)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/05/20 12:01:57 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) (192.168.1.134 executor driver): org.apache.spark.SparkException: \n",
      "Error from python worker:\n",
      "  Error: pyspark does not support any application options.\n",
      "  \n",
      "  Usage: ./bin/pyspark [options]\n",
      "  \n",
      "  Options:\n",
      "    --master MASTER_URL         spark://host:port, mesos://host:port, yarn,\n",
      "                                k8s://https://host:port, or local (Default: local[*]).\n",
      "    --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\n",
      "                                on one of the worker machines inside the cluster (\"cluster\")\n",
      "                                (Default: client).\n",
      "    --class CLASS_NAME          Your application's main class (for Java / Scala apps).\n",
      "    --name NAME                 A name of your application.\n",
      "    --jars JARS                 Comma-separated list of jars to include on the driver\n",
      "                                and executor classpaths.\n",
      "    --packages                  Comma-separated list of maven coordinates of jars to include\n",
      "                                on the driver and executor classpaths. Will search the local\n",
      "                                maven repo, then maven central and any additional remote\n",
      "                                repositories given by --repositories. The format for the\n",
      "                                coordinates should be groupId:artifactId:version.\n",
      "    --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while\n",
      "                                resolving the dependencies provided in --packages to avoid\n",
      "                                dependency conflicts.\n",
      "    --repositories              Comma-separated list of additional remote repositories to\n",
      "                                search for the maven coordinates given with --packages.\n",
      "    --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place\n",
      "                                on the PYTHONPATH for Python apps.\n",
      "    --files FILES               Comma-separated list of files to be placed in the working\n",
      "                                directory of each executor. File paths of these files\n",
      "                                in executors can be accessed via SparkFiles.get(fileName).\n",
      "    --archives ARCHIVES         Comma-separated list of archives to be extracted into the\n",
      "                                working directory of each executor.\n",
      "  \n",
      "    --conf, -c PROP=VALUE       Arbitrary Spark configuration property.\n",
      "    --properties-file FILE      Path to a file from which to load extra properties. If not\n",
      "                                specified, this will look for conf/spark-defaults.conf.\n",
      "  \n",
      "    --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n",
      "    --driver-java-options       Extra Java options to pass to the driver.\n",
      "    --driver-library-path       Extra library path entries to pass to the driver.\n",
      "    --driver-class-path         Extra class path entries to pass to the driver. Note that\n",
      "                                jars added with --jars are automatically included in the\n",
      "                                classpath.\n",
      "  \n",
      "    --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n",
      "  \n",
      "    --proxy-user NAME           User to impersonate when submitting the application.\n",
      "                                This argument does not work with --principal / --keytab.\n",
      "  \n",
      "    --help, -h                  Show this help message and exit.\n",
      "    --verbose, -v               Print additional debug output.\n",
      "    --version,                  Print the version of current Spark.\n",
      "  \n",
      "   Spark Connect only:\n",
      "     --remote CONNECT_URL       URL to connect to the server for Spark Connect, e.g.,\n",
      "                                sc://host:port. --master and --deploy-mode cannot be set\n",
      "                                together with this option. This option is experimental, and\n",
      "                                might change between minor releases.\n",
      "  \n",
      "   Cluster deploy mode only:\n",
      "    --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n",
      "                                (Default: 1).\n",
      "  \n",
      "   Spark standalone or Mesos with cluster deploy mode only:\n",
      "    --supervise                 If given, restarts the driver on failure.\n",
      "  \n",
      "   Spark standalone, Mesos or K8s with cluster deploy mode only:\n",
      "    --kill SUBMISSION_ID        If given, kills the driver specified.\n",
      "    --status SUBMISSION_ID      If given, requests the status of the driver specified.\n",
      "  \n",
      "   Spark standalone, Mesos and Kubernetes only:\n",
      "    --total-executor-cores NUM  Total cores for all executors.\n",
      "  \n",
      "   Spark standalone, YARN and Kubernetes only:\n",
      "    --executor-cores NUM        Number of cores used by each executor. (Default: 1 in\n",
      "                                YARN and K8S modes, or all available cores on the worker\n",
      "                                in standalone mode).\n",
      "  \n",
      "   Spark on YARN and Kubernetes only:\n",
      "    --num-executors NUM         Number of executors to launch (Default: 2).\n",
      "                                If dynamic allocation is enabled, the initial number of\n",
      "                                executors will be at least NUM.\n",
      "    --principal PRINCIPAL       Principal to be used to login to KDC.\n",
      "    --keytab KEYTAB             The full path to the file that contains the keytab for the\n",
      "                                principal specified above.\n",
      "  \n",
      "   Spark on YARN only:\n",
      "    --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n",
      "        \n",
      "PYTHONPATH was:\n",
      "  /Users/asaucedov/Downloads/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip:/Users/asaucedov/Downloads/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip:/Users/asaucedov/Downloads/spark-3.5.0-bin-hadoop3/jars/spark-core_2.12-3.5.0.jar\n",
      "org.apache.spark.SparkException: EOFException occurred while reading the port number from pyspark.daemon's stdout and terminated with code: 1..\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.eofExceptionWhileReadPortNumberError(SparkCoreErrors.scala:53)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:250)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/05/20 12:01:57 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.ml.stat.ChiSquareTest.test.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (192.168.1.134 executor driver): org.apache.spark.SparkException: \nError from python worker:\n  Error: pyspark does not support any application options.\n  \n  Usage: ./bin/pyspark [options]\n  \n  Options:\n    --master MASTER_URL         spark://host:port, mesos://host:port, yarn,\n                                k8s://https://host:port, or local (Default: local[*]).\n    --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\n                                on one of the worker machines inside the cluster (\"cluster\")\n                                (Default: client).\n    --class CLASS_NAME          Your application's main class (for Java / Scala apps).\n    --name NAME                 A name of your application.\n    --jars JARS                 Comma-separated list of jars to include on the driver\n                                and executor classpaths.\n    --packages                  Comma-separated list of maven coordinates of jars to include\n                                on the driver and executor classpaths. Will search the local\n                                maven repo, then maven central and any additional remote\n                                repositories given by --repositories. The format for the\n                                coordinates should be groupId:artifactId:version.\n    --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while\n                                resolving the dependencies provided in --packages to avoid\n                                dependency conflicts.\n    --repositories              Comma-separated list of additional remote repositories to\n                                search for the maven coordinates given with --packages.\n    --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place\n                                on the PYTHONPATH for Python apps.\n    --files FILES               Comma-separated list of files to be placed in the working\n                                directory of each executor. File paths of these files\n                                in executors can be accessed via SparkFiles.get(fileName).\n    --archives ARCHIVES         Comma-separated list of archives to be extracted into the\n                                working directory of each executor.\n  \n    --conf, -c PROP=VALUE       Arbitrary Spark configuration property.\n    --properties-file FILE      Path to a file from which to load extra properties. If not\n                                specified, this will look for conf/spark-defaults.conf.\n  \n    --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n    --driver-java-options       Extra Java options to pass to the driver.\n    --driver-library-path       Extra library path entries to pass to the driver.\n    --driver-class-path         Extra class path entries to pass to the driver. Note that\n                                jars added with --jars are automatically included in the\n                                classpath.\n  \n    --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n  \n    --proxy-user NAME           User to impersonate when submitting the application.\n                                This argument does not work with --principal / --keytab.\n  \n    --help, -h                  Show this help message and exit.\n    --verbose, -v               Print additional debug output.\n    --version,                  Print the version of current Spark.\n  \n   Spark Connect only:\n     --remote CONNECT_URL       URL to connect to the server for Spark Connect, e.g.,\n                                sc://host:port. --master and --deploy-mode cannot be set\n                                together with this option. This option is experimental, and\n                                might change between minor releases.\n  \n   Cluster deploy mode only:\n    --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n                                (Default: 1).\n  \n   Spark standalone or Mesos with cluster deploy mode only:\n    --supervise                 If given, restarts the driver on failure.\n  \n   Spark standalone, Mesos or K8s with cluster deploy mode only:\n    --kill SUBMISSION_ID        If given, kills the driver specified.\n    --status SUBMISSION_ID      If given, requests the status of the driver specified.\n  \n   Spark standalone, Mesos and Kubernetes only:\n    --total-executor-cores NUM  Total cores for all executors.\n  \n   Spark standalone, YARN and Kubernetes only:\n    --executor-cores NUM        Number of cores used by each executor. (Default: 1 in\n                                YARN and K8S modes, or all available cores on the worker\n                                in standalone mode).\n  \n   Spark on YARN and Kubernetes only:\n    --num-executors NUM         Number of executors to launch (Default: 2).\n                                If dynamic allocation is enabled, the initial number of\n                                executors will be at least NUM.\n    --principal PRINCIPAL       Principal to be used to login to KDC.\n    --keytab KEYTAB             The full path to the file that contains the keytab for the\n                                principal specified above.\n  \n   Spark on YARN only:\n    --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n        \nPYTHONPATH was:\n  /Users/asaucedov/Downloads/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip:/Users/asaucedov/Downloads/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip:/Users/asaucedov/Downloads/spark-3.5.0-bin-hadoop3/jars/spark-core_2.12-3.5.0.jar\norg.apache.spark.SparkException: EOFException occurred while reading the port number from pyspark.daemon's stdout and terminated with code: 1..\n\tat org.apache.spark.errors.SparkCoreErrors$.eofExceptionWhileReadPortNumberError(SparkCoreErrors.scala:53)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:250)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1489)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1462)\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1503)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1503)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSquared(ChiSqTest.scala:95)\n\tat org.apache.spark.ml.stat.ChiSquareTest$.test(ChiSquareTest.scala:84)\n\tat org.apache.spark.ml.stat.ChiSquareTest.test(ChiSquareTest.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: \nError from python worker:\n  Error: pyspark does not support any application options.\n  \n  Usage: ./bin/pyspark [options]\n  \n  Options:\n    --master MASTER_URL         spark://host:port, mesos://host:port, yarn,\n                                k8s://https://host:port, or local (Default: local[*]).\n    --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\n                                on one of the worker machines inside the cluster (\"cluster\")\n                                (Default: client).\n    --class CLASS_NAME          Your application's main class (for Java / Scala apps).\n    --name NAME                 A name of your application.\n    --jars JARS                 Comma-separated list of jars to include on the driver\n                                and executor classpaths.\n    --packages                  Comma-separated list of maven coordinates of jars to include\n                                on the driver and executor classpaths. Will search the local\n                                maven repo, then maven central and any additional remote\n                                repositories given by --repositories. The format for the\n                                coordinates should be groupId:artifactId:version.\n    --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while\n                                resolving the dependencies provided in --packages to avoid\n                                dependency conflicts.\n    --repositories              Comma-separated list of additional remote repositories to\n                                search for the maven coordinates given with --packages.\n    --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place\n                                on the PYTHONPATH for Python apps.\n    --files FILES               Comma-separated list of files to be placed in the working\n                                directory of each executor. File paths of these files\n                                in executors can be accessed via SparkFiles.get(fileName).\n    --archives ARCHIVES         Comma-separated list of archives to be extracted into the\n                                working directory of each executor.\n  \n    --conf, -c PROP=VALUE       Arbitrary Spark configuration property.\n    --properties-file FILE      Path to a file from which to load extra properties. If not\n                                specified, this will look for conf/spark-defaults.conf.\n  \n    --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n    --driver-java-options       Extra Java options to pass to the driver.\n    --driver-library-path       Extra library path entries to pass to the driver.\n    --driver-class-path         Extra class path entries to pass to the driver. Note that\n                                jars added with --jars are automatically included in the\n                                classpath.\n  \n    --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n  \n    --proxy-user NAME           User to impersonate when submitting the application.\n                                This argument does not work with --principal / --keytab.\n  \n    --help, -h                  Show this help message and exit.\n    --verbose, -v               Print additional debug output.\n    --version,                  Print the version of current Spark.\n  \n   Spark Connect only:\n     --remote CONNECT_URL       URL to connect to the server for Spark Connect, e.g.,\n                                sc://host:port. --master and --deploy-mode cannot be set\n                                together with this option. This option is experimental, and\n                                might change between minor releases.\n  \n   Cluster deploy mode only:\n    --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n                                (Default: 1).\n  \n   Spark standalone or Mesos with cluster deploy mode only:\n    --supervise                 If given, restarts the driver on failure.\n  \n   Spark standalone, Mesos or K8s with cluster deploy mode only:\n    --kill SUBMISSION_ID        If given, kills the driver specified.\n    --status SUBMISSION_ID      If given, requests the status of the driver specified.\n  \n   Spark standalone, Mesos and Kubernetes only:\n    --total-executor-cores NUM  Total cores for all executors.\n  \n   Spark standalone, YARN and Kubernetes only:\n    --executor-cores NUM        Number of cores used by each executor. (Default: 1 in\n                                YARN and K8S modes, or all available cores on the worker\n                                in standalone mode).\n  \n   Spark on YARN and Kubernetes only:\n    --num-executors NUM         Number of executors to launch (Default: 2).\n                                If dynamic allocation is enabled, the initial number of\n                                executors will be at least NUM.\n    --principal PRINCIPAL       Principal to be used to login to KDC.\n    --keytab KEYTAB             The full path to the file that contains the keytab for the\n                                principal specified above.\n  \n   Spark on YARN only:\n    --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n        \nPYTHONPATH was:\n  /Users/asaucedov/Downloads/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip:/Users/asaucedov/Downloads/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip:/Users/asaucedov/Downloads/spark-3.5.0-bin-hadoop3/jars/spark-core_2.12-3.5.0.jar\norg.apache.spark.SparkException: EOFException occurred while reading the port number from pyspark.daemon's stdout and terminated with code: 1..\n\tat org.apache.spark.errors.SparkCoreErrors$.eofExceptionWhileReadPortNumberError(SparkCoreErrors.scala:53)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:250)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mChiSquareTest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n\u001b[1;32m      2\u001b[0m r\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/ml/stat.py:112\u001b[0m, in \u001b[0;36mChiSquareTest.test\u001b[0;34m(dataset, featuresCol, labelCol, flatten)\u001b[0m\n\u001b[1;32m    110\u001b[0m javaTestObj \u001b[38;5;241m=\u001b[39m _jvm()\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mml\u001b[38;5;241m.\u001b[39mstat\u001b[38;5;241m.\u001b[39mChiSquareTest\n\u001b[1;32m    111\u001b[0m args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m (dataset, featuresCol, labelCol, flatten)]\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _java2py(sc, \u001b[43mjavaTestObj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.ml.stat.ChiSquareTest.test.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (192.168.1.134 executor driver): org.apache.spark.SparkException: \nError from python worker:\n  Error: pyspark does not support any application options.\n  \n  Usage: ./bin/pyspark [options]\n  \n  Options:\n    --master MASTER_URL         spark://host:port, mesos://host:port, yarn,\n                                k8s://https://host:port, or local (Default: local[*]).\n    --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\n                                on one of the worker machines inside the cluster (\"cluster\")\n                                (Default: client).\n    --class CLASS_NAME          Your application's main class (for Java / Scala apps).\n    --name NAME                 A name of your application.\n    --jars JARS                 Comma-separated list of jars to include on the driver\n                                and executor classpaths.\n    --packages                  Comma-separated list of maven coordinates of jars to include\n                                on the driver and executor classpaths. Will search the local\n                                maven repo, then maven central and any additional remote\n                                repositories given by --repositories. The format for the\n                                coordinates should be groupId:artifactId:version.\n    --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while\n                                resolving the dependencies provided in --packages to avoid\n                                dependency conflicts.\n    --repositories              Comma-separated list of additional remote repositories to\n                                search for the maven coordinates given with --packages.\n    --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place\n                                on the PYTHONPATH for Python apps.\n    --files FILES               Comma-separated list of files to be placed in the working\n                                directory of each executor. File paths of these files\n                                in executors can be accessed via SparkFiles.get(fileName).\n    --archives ARCHIVES         Comma-separated list of archives to be extracted into the\n                                working directory of each executor.\n  \n    --conf, -c PROP=VALUE       Arbitrary Spark configuration property.\n    --properties-file FILE      Path to a file from which to load extra properties. If not\n                                specified, this will look for conf/spark-defaults.conf.\n  \n    --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n    --driver-java-options       Extra Java options to pass to the driver.\n    --driver-library-path       Extra library path entries to pass to the driver.\n    --driver-class-path         Extra class path entries to pass to the driver. Note that\n                                jars added with --jars are automatically included in the\n                                classpath.\n  \n    --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n  \n    --proxy-user NAME           User to impersonate when submitting the application.\n                                This argument does not work with --principal / --keytab.\n  \n    --help, -h                  Show this help message and exit.\n    --verbose, -v               Print additional debug output.\n    --version,                  Print the version of current Spark.\n  \n   Spark Connect only:\n     --remote CONNECT_URL       URL to connect to the server for Spark Connect, e.g.,\n                                sc://host:port. --master and --deploy-mode cannot be set\n                                together with this option. This option is experimental, and\n                                might change between minor releases.\n  \n   Cluster deploy mode only:\n    --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n                                (Default: 1).\n  \n   Spark standalone or Mesos with cluster deploy mode only:\n    --supervise                 If given, restarts the driver on failure.\n  \n   Spark standalone, Mesos or K8s with cluster deploy mode only:\n    --kill SUBMISSION_ID        If given, kills the driver specified.\n    --status SUBMISSION_ID      If given, requests the status of the driver specified.\n  \n   Spark standalone, Mesos and Kubernetes only:\n    --total-executor-cores NUM  Total cores for all executors.\n  \n   Spark standalone, YARN and Kubernetes only:\n    --executor-cores NUM        Number of cores used by each executor. (Default: 1 in\n                                YARN and K8S modes, or all available cores on the worker\n                                in standalone mode).\n  \n   Spark on YARN and Kubernetes only:\n    --num-executors NUM         Number of executors to launch (Default: 2).\n                                If dynamic allocation is enabled, the initial number of\n                                executors will be at least NUM.\n    --principal PRINCIPAL       Principal to be used to login to KDC.\n    --keytab KEYTAB             The full path to the file that contains the keytab for the\n                                principal specified above.\n  \n   Spark on YARN only:\n    --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n        \nPYTHONPATH was:\n  /Users/asaucedov/Downloads/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip:/Users/asaucedov/Downloads/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip:/Users/asaucedov/Downloads/spark-3.5.0-bin-hadoop3/jars/spark-core_2.12-3.5.0.jar\norg.apache.spark.SparkException: EOFException occurred while reading the port number from pyspark.daemon's stdout and terminated with code: 1..\n\tat org.apache.spark.errors.SparkCoreErrors$.eofExceptionWhileReadPortNumberError(SparkCoreErrors.scala:53)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:250)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1489)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1462)\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1503)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1503)\n\tat org.apache.spark.mllib.stat.test.ChiSqTest$.computeChiSquared(ChiSqTest.scala:95)\n\tat org.apache.spark.ml.stat.ChiSquareTest$.test(ChiSquareTest.scala:84)\n\tat org.apache.spark.ml.stat.ChiSquareTest.test(ChiSquareTest.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: \nError from python worker:\n  Error: pyspark does not support any application options.\n  \n  Usage: ./bin/pyspark [options]\n  \n  Options:\n    --master MASTER_URL         spark://host:port, mesos://host:port, yarn,\n                                k8s://https://host:port, or local (Default: local[*]).\n    --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\n                                on one of the worker machines inside the cluster (\"cluster\")\n                                (Default: client).\n    --class CLASS_NAME          Your application's main class (for Java / Scala apps).\n    --name NAME                 A name of your application.\n    --jars JARS                 Comma-separated list of jars to include on the driver\n                                and executor classpaths.\n    --packages                  Comma-separated list of maven coordinates of jars to include\n                                on the driver and executor classpaths. Will search the local\n                                maven repo, then maven central and any additional remote\n                                repositories given by --repositories. The format for the\n                                coordinates should be groupId:artifactId:version.\n    --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while\n                                resolving the dependencies provided in --packages to avoid\n                                dependency conflicts.\n    --repositories              Comma-separated list of additional remote repositories to\n                                search for the maven coordinates given with --packages.\n    --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place\n                                on the PYTHONPATH for Python apps.\n    --files FILES               Comma-separated list of files to be placed in the working\n                                directory of each executor. File paths of these files\n                                in executors can be accessed via SparkFiles.get(fileName).\n    --archives ARCHIVES         Comma-separated list of archives to be extracted into the\n                                working directory of each executor.\n  \n    --conf, -c PROP=VALUE       Arbitrary Spark configuration property.\n    --properties-file FILE      Path to a file from which to load extra properties. If not\n                                specified, this will look for conf/spark-defaults.conf.\n  \n    --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n    --driver-java-options       Extra Java options to pass to the driver.\n    --driver-library-path       Extra library path entries to pass to the driver.\n    --driver-class-path         Extra class path entries to pass to the driver. Note that\n                                jars added with --jars are automatically included in the\n                                classpath.\n  \n    --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n  \n    --proxy-user NAME           User to impersonate when submitting the application.\n                                This argument does not work with --principal / --keytab.\n  \n    --help, -h                  Show this help message and exit.\n    --verbose, -v               Print additional debug output.\n    --version,                  Print the version of current Spark.\n  \n   Spark Connect only:\n     --remote CONNECT_URL       URL to connect to the server for Spark Connect, e.g.,\n                                sc://host:port. --master and --deploy-mode cannot be set\n                                together with this option. This option is experimental, and\n                                might change between minor releases.\n  \n   Cluster deploy mode only:\n    --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n                                (Default: 1).\n  \n   Spark standalone or Mesos with cluster deploy mode only:\n    --supervise                 If given, restarts the driver on failure.\n  \n   Spark standalone, Mesos or K8s with cluster deploy mode only:\n    --kill SUBMISSION_ID        If given, kills the driver specified.\n    --status SUBMISSION_ID      If given, requests the status of the driver specified.\n  \n   Spark standalone, Mesos and Kubernetes only:\n    --total-executor-cores NUM  Total cores for all executors.\n  \n   Spark standalone, YARN and Kubernetes only:\n    --executor-cores NUM        Number of cores used by each executor. (Default: 1 in\n                                YARN and K8S modes, or all available cores on the worker\n                                in standalone mode).\n  \n   Spark on YARN and Kubernetes only:\n    --num-executors NUM         Number of executors to launch (Default: 2).\n                                If dynamic allocation is enabled, the initial number of\n                                executors will be at least NUM.\n    --principal PRINCIPAL       Principal to be used to login to KDC.\n    --keytab KEYTAB             The full path to the file that contains the keytab for the\n                                principal specified above.\n  \n   Spark on YARN only:\n    --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n        \nPYTHONPATH was:\n  /Users/asaucedov/Downloads/spark-3.5.0-bin-hadoop3/python/lib/pyspark.zip:/Users/asaucedov/Downloads/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip:/Users/asaucedov/Downloads/spark-3.5.0-bin-hadoop3/jars/spark-core_2.12-3.5.0.jar\norg.apache.spark.SparkException: EOFException occurred while reading the port number from pyspark.daemon's stdout and terminated with code: 1..\n\tat org.apache.spark.errors.SparkCoreErrors$.eofExceptionWhileReadPortNumberError(SparkCoreErrors.scala:53)\n\tat org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:250)\n\tat org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:139)\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "r = ChiSquareTest.test(df, 'features', 'label').head()\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
